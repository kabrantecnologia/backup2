#!/usr/bin/env python3
"""
Generate SQL migration files to import data from CSVs into existing tables:
- public.gs1_api_responses
- public.marketplace_products
- public.marketplace_product_images

It reads these CSV files (paths are absolute below) and writes 3 migration files
into `tricket-backend/supabase/migrations/` with the next sequence numbers:
  28_import_gs1_api_responses.sql
  29_import_marketplace_products.sql
  30_import_marketplace_product_images.sql

Notes:
- Uses INSERT ... ON CONFLICT to be idempotent.
- Escapes text/JSON safely for SQL single-quoted literals.
- Casts JSON to ::jsonb and timestamps to ::timestamptz where applicable.
- Numeric fields are inserted as numeric literals when present, otherwise NULL.

Run:
  python3 storage-migration/generate_csv_inserts.py
"""
import csv
import os
from datetime import datetime
from typing import Optional

BASE = "/home/joaohenrique/workspaces/projects/tricket"
CSV_GS1 = os.path.join(BASE, "storage-migration/gs1_api_responses_rows.csv")
CSV_PRODUCTS = os.path.join(BASE, "storage-migration/marketplace_products_rows.csv")
CSV_IMAGES = os.path.join(BASE, "storage-migration/marketplace_product_images_rows.csv")
CSV_BRANDS = os.path.join(BASE, "storage-migration/marketplace_brands_rows.csv")
CSV_GPCMAP = os.path.join(BASE, "storage-migration/marketplace_gpc_to_tricket_category_mapping_rows.csv")
MIGRATIONS_DIR = os.path.join(BASE, "tricket-backend/supabase/migrations")

OUT_BRANDS = os.path.join(MIGRATIONS_DIR, "28_import_marketplace_brands.sql")
OUT_GS1 = os.path.join(MIGRATIONS_DIR, "29_import_gs1_api_responses.sql")
OUT_PRODUCTS = os.path.join(MIGRATIONS_DIR, "30_import_marketplace_products.sql")
OUT_IMAGES = os.path.join(MIGRATIONS_DIR, "31_import_marketplace_product_images.sql")
OUT_NORMALIZE = os.path.join(MIGRATIONS_DIR, "32_normalize_product_names.sql")
OUT_GPCMAP = os.path.join(MIGRATIONS_DIR, "33_import_gpc_to_tricket_category_mapping.sql")


def sqlesc(value: Optional[str]) -> str:
    """Escape a Python string for single-quoted SQL literal, or return NULL."""
    if value is None:
        return "NULL"
    s = str(value)
    if s == "":
        return "NULL"
    return "'" + s.replace("'", "''") + "'"


def sqljsonb(json_text: Optional[str]) -> str:
    if json_text is None or json_text == "":
        return "NULL"
    # The CSV stores JSON text already. We wrap as a single-quoted string and cast to jsonb
    return f"{sqlesc(json_text)}::jsonb"


def sqlnum(s: Optional[str]) -> str:
    if s is None or s == "":
        return "NULL"
    return s  # assume CSV contains a valid numeric literal


def sqlts(ts: Optional[str]) -> str:
    if ts is None or ts == "":
        return "NULL"
    # Trust the incoming timestamptz format
    return f"{sqlesc(ts)}::timestamptz"


def write_header(f, title: str):
    f.write("/" + "*"*118 + "\n")
    f.write(f"*   AUTO-GENERATED IMPORT: {title}\n")
    f.write("*   This file was generated by storage-migration/generate_csv_inserts.py\n")
    f.write("*   It inserts/updates records based on CSV content.\n")
    f.write("""*   Idempotent via ON CONFLICT.
""")
    f.write("""**********************************************************************************************************************/

BEGIN;

""")


def write_footer(f):
    f.write("\nCOMMIT;\n")


def generate_gs1():
    with open(CSV_GS1, newline="", encoding="utf-8") as cf, open(OUT_GS1, "w", encoding="utf-8") as out:
        reader = csv.DictReader(cf)
        write_header(out, "GS1 API Responses -> public.gs1_api_responses")

        out.write("-- Columns: id, created_at, gtin, raw_response, status, error_message, created_by_user_id, processed_at\n")
        for row in reader:
            id_ = row.get("id")
            created_at = row.get("created_at")
            gtin = row.get("gtin")
            raw_response = row.get("raw_response")
            status = row.get("status")
            error_message = row.get("error_message")
            created_by_user_id = row.get("created_by_user_id")
            processed_at = row.get("processed_at")

            out.write(
                "INSERT INTO public.gs1_api_responses (id, created_at, gtin, raw_response, status, error_message, created_by_user_id, processed_at)\n"
                "VALUES ("
                f"{sqlesc(id_)} , {sqlts(created_at)} , {sqlesc(gtin)} , {sqljsonb(raw_response)} , {sqlesc(status)} , {sqlesc(error_message)} , {sqlesc(created_by_user_id)} , {sqlts(processed_at)}"
                ")\n"
                "ON CONFLICT (id) DO UPDATE SET\n"
                "  created_at = EXCLUDED.created_at,\n"
                "  gtin = EXCLUDED.gtin,\n"
                "  raw_response = EXCLUDED.raw_response,\n"
                "  status = EXCLUDED.status,\n"
                "  error_message = EXCLUDED.error_message,\n"
                "  created_by_user_id = EXCLUDED.created_by_user_id,\n"
                "  processed_at = EXCLUDED.processed_at;\n\n"
            )

        write_footer(out)


def generate_gpc_mapping():
    with open(CSV_GPCMAP, newline="", encoding="utf-8") as cf, open(OUT_GPCMAP, "w", encoding="utf-8") as out:
        reader = csv.DictReader(cf)
        write_header(out, "Import marketplace_gpc_to_tricket_category_mapping from CSV with upsert")
        for row in reader:
            gpc_category_code = row.get("gpc_category_code")
            gpc_category_name = row.get("gpc_category_name")
            tricket_sub_category_id = row.get("tricket_sub_category_id") or None
            status = row.get("status")
            created_at = row.get("created_at")
            updated_at = row.get("updated_at")

            out.write(
                "INSERT INTO public.marketplace_gpc_to_tricket_category_mapping (\n"
                "  gpc_category_code, gpc_category_name, tricket_sub_category_id, status, created_at, updated_at\n"
                ") VALUES (\n"
                f"  {sqlesc(gpc_category_code)}, {sqlesc(gpc_category_name)}, {sqlesc(tricket_sub_category_id)}, {sqlesc(status)}, {sqlts(created_at)}, {sqlts(updated_at)}\n"
                ")\n"
                "ON CONFLICT (gpc_category_code) DO UPDATE SET\n"
                "  gpc_category_name = EXCLUDED.gpc_category_name,\n"
                "  tricket_sub_category_id = EXCLUDED.tricket_sub_category_id,\n"
                "  status = EXCLUDED.status,\n"
                "  created_at = EXCLUDED.created_at,\n"
                "  updated_at = EXCLUDED.updated_at;\n\n"
            )
        write_footer(out)


def generate_normalize():
    with open(OUT_NORMALIZE, "w", encoding="utf-8") as out:
        write_header(out, "Normalize marketplace_products names to UPPERCASE")
        out.write("-- Normalize names to uppercase, trimming surrounding spaces first\n")
        out.write("UPDATE public.marketplace_products\n")
        out.write("SET name = UPPER(BTRIM(name))\n")
        out.write("WHERE name IS NOT NULL;\n")
        write_footer(out)


def generate_brands():
    with open(CSV_BRANDS, newline="", encoding="utf-8") as cf, open(OUT_BRANDS, "w", encoding="utf-8") as out:
        reader = csv.DictReader(cf)
        write_header(out, "Marketplace Brands -> public.marketplace_brands")

        out.write("-- Columns follow 18_marketplace_catalogo_ofertas.sql marketplace_brands definition\n")
        for r in reader:
            id_ = r.get("id")
            name = r.get("name")
            slug = r.get("slug")
            description = r.get("description")
            logo_url = r.get("logo_url")
            official_website = r.get("official_website")
            country_of_origin_code = r.get("country_of_origin_code")
            gs1_company_prefix = r.get("gs1_company_prefix")
            gln_brand_owner = r.get("gln_brand_owner")
            status = r.get("status")
            approved_by_user_id = r.get("approved_by_user_id")
            approved_at = r.get("approved_at")
            created_by_user_id = r.get("created_by_user_id")  # ignored: we will force NULL to avoid FK issues
            created_at = r.get("created_at")
            updated_at = r.get("updated_at")

            out.write(
                "INSERT INTO public.marketplace_brands (\n"
                "  id, name, slug, description, logo_url, official_website, country_of_origin_code, gs1_company_prefix, gln_brand_owner,\n"
                "  status, approved_by_user_id, approved_at, created_by_user_id, created_at, updated_at\n"
                ") VALUES (\n"
                f"  {sqlesc(id_)}, {sqlesc(name)}, {sqlesc(slug)}, {sqlesc(description)}, {sqlesc(logo_url)}, {sqlesc(official_website)}, {sqlesc(country_of_origin_code)}, {sqlesc(gs1_company_prefix)}, {sqlesc(gln_brand_owner)},\n"
                f"  {sqlesc(status)}, {sqlesc(approved_by_user_id)}, {sqlts(approved_at)}, {sqlesc(created_by_user_id)}, {sqlts(created_at)}, {sqlts(updated_at)}\n"
                ")\n"
                "ON CONFLICT (name) DO UPDATE SET\n"
                "  slug = EXCLUDED.slug,\n"
                "  description = EXCLUDED.description,\n"
                "  logo_url = EXCLUDED.logo_url,\n"
                "  official_website = EXCLUDED.official_website,\n"
                "  country_of_origin_code = EXCLUDED.country_of_origin_code,\n"
                "  gs1_company_prefix = EXCLUDED.gs1_company_prefix,\n"
                "  gln_brand_owner = EXCLUDED.gln_brand_owner,\n"
                "  status = EXCLUDED.status,\n"
                "  approved_by_user_id = EXCLUDED.approved_by_user_id,\n"
                "  approved_at = EXCLUDED.approved_at,\n"
                "  created_by_user_id = EXCLUDED.created_by_user_id,\n"
                "  created_at = EXCLUDED.created_at,\n"
                "  updated_at = EXCLUDED.updated_at;\n\n"
            )

        write_footer(out)


def generate_products():
    with open(CSV_PRODUCTS, newline="", encoding="utf-8") as cf, open(OUT_PRODUCTS, "w", encoding="utf-8") as out:
        reader = csv.DictReader(cf)
        write_header(out, "Marketplace Products -> public.marketplace_products")

        out.write("-- Columns must match the table definition in 18_marketplace_catalogo_ofertas.sql\n")
        for r in reader:
            # Map CSV fields
            id_ = r.get("id")
            sub_category_id = r.get("sub_category_id")
            brand_id = r.get("brand_id")
            name = r.get("name")
            description = r.get("description")
            sku_base = r.get("sku_base")
            attributes = r.get("attributes")
            status = r.get("status")
            gtin = r.get("gtin")
            gpc_category_code = r.get("gpc_category_code")
            ncm_code = r.get("ncm_code")
            cest_code = r.get("cest_code")
            net_content = r.get("net_content")
            net_content_unit = r.get("net_content_unit")
            gross_weight = r.get("gross_weight")
            net_weight = r.get("net_weight")
            weight_unit = r.get("weight_unit")
            height = r.get("height")
            width = r.get("width")
            depth = r.get("depth")
            dimension_unit = r.get("dimension_unit")
            country_of_origin_code = r.get("country_of_origin_code")
            gs1_company_name = r.get("gs1_company_name")
            gs1_company_gln = r.get("gs1_company_gln")
            created_by_user_id = r.get("created_by_user_id")
            created_at = r.get("created_at")
            updated_at = r.get("updated_at")

            out.write(
                "INSERT INTO public.marketplace_products (\n"
                "  id, sub_category_id, brand_id, name, description, sku_base, attributes, status, gtin,\n"
                "  gpc_category_code, ncm_code, cest_code, net_content, net_content_unit, gross_weight, net_weight,\n"
                "  weight_unit, height, width, depth, dimension_unit, country_of_origin_code, gs1_company_name, gs1_company_gln,\n"
                "  created_by_user_id, created_at, updated_at\n"
                ") VALUES (\n"
                f"  {sqlesc(id_)}, {sqlesc(sub_category_id)}, {sqlesc(brand_id)}, UPPER({sqlesc(name)}), {sqlesc(description)}, {sqlesc(sku_base)}, {sqljsonb(attributes)}, {sqlesc(status)}, {sqlesc(gtin)},\n"
                f"  {sqlesc(gpc_category_code)}, {sqlesc(ncm_code)}, {sqlesc(cest_code)}, {sqlnum(net_content)}, {sqlesc(net_content_unit)}, {sqlnum(gross_weight)}, {sqlnum(net_weight)},\n"
                f"  {sqlesc(weight_unit)}, {sqlnum(height)}, {sqlnum(width)}, {sqlnum(depth)}, {sqlesc(dimension_unit)}, {sqlesc(country_of_origin_code)}, {sqlesc(gs1_company_name)}, {sqlesc(gs1_company_gln)},\n"
                f"  NULL, {sqlts(created_at)}, {sqlts(updated_at)}\n"
                ")\n"
                "ON CONFLICT (gtin) DO UPDATE SET\n"
                "  sub_category_id = EXCLUDED.sub_category_id,\n"
                "  brand_id = EXCLUDED.brand_id,\n"
                "  name = UPPER(EXCLUDED.name),\n"
                "  description = EXCLUDED.description,\n"
                "  sku_base = EXCLUDED.sku_base,\n"
                "  attributes = EXCLUDED.attributes,\n"
                "  status = EXCLUDED.status,\n"
                "  gpc_category_code = EXCLUDED.gpc_category_code,\n"
                "  ncm_code = EXCLUDED.ncm_code,\n"
                "  cest_code = EXCLUDED.cest_code,\n"
                "  net_content = EXCLUDED.net_content,\n"
                "  net_content_unit = EXCLUDED.net_content_unit,\n"
                "  gross_weight = EXCLUDED.gross_weight,\n"
                "  net_weight = EXCLUDED.net_weight,\n"
                "  weight_unit = EXCLUDED.weight_unit,\n"
                "  height = EXCLUDED.height,\n"
                "  width = EXCLUDED.width,\n"
                "  depth = EXCLUDED.depth,\n"
                "  dimension_unit = EXCLUDED.dimension_unit,\n"
                "  country_of_origin_code = EXCLUDED.country_of_origin_code,\n"
                "  gs1_company_name = EXCLUDED.gs1_company_name,\n"
                "  gs1_company_gln = EXCLUDED.gs1_company_gln,\n"
                "  created_at = EXCLUDED.created_at,\n"
                "  updated_at = EXCLUDED.updated_at;\n\n"
            )

        write_footer(out)


def generate_images():
    with open(CSV_IMAGES, newline="", encoding="utf-8") as cf, open(OUT_IMAGES, "w", encoding="utf-8") as out:
        reader = csv.DictReader(cf)
        write_header(out, "Marketplace Product Images -> public.marketplace_product_images")

        out.write("-- Columns: id, product_id, image_url, alt_text, sort_order, image_type_code, created_at\n")
        for r in reader:
            id_ = r.get("id")
            product_id = r.get("product_id")
            image_url = r.get("image_url")
            alt_text = r.get("alt_text")
            sort_order = r.get("sort_order")
            image_type_code = r.get("image_type_code")
            created_at = r.get("created_at")

            out.write(
                "INSERT INTO public.marketplace_product_images (id, product_id, image_url, alt_text, sort_order, image_type_code, created_at)\n"
                "VALUES ("
                f"{sqlesc(id_)}, {sqlesc(product_id)}, {sqlesc(image_url)}, {sqlesc(alt_text)}, {sqlnum(sort_order)}, {sqlesc(image_type_code)}, {sqlts(created_at)}"
                ")\n"
                "ON CONFLICT (id) DO UPDATE SET\n"
                "  product_id = EXCLUDED.product_id,\n"
                "  image_url = EXCLUDED.image_url,\n"
                "  alt_text = EXCLUDED.alt_text,\n"
                "  sort_order = EXCLUDED.sort_order,\n"
                "  image_type_code = EXCLUDED.image_type_code,\n"
                "  created_at = EXCLUDED.created_at;\n\n"
            )

        write_footer(out)


def main():
    # Basic checks
    for p in (CSV_GS1, CSV_PRODUCTS, CSV_IMAGES, CSV_BRANDS):
        if not os.path.isfile(p):
            raise FileNotFoundError(p)
    os.makedirs(MIGRATIONS_DIR, exist_ok=True)

    generate_gs1()
    generate_products()
    generate_images()
    generate_brands()
    generate_normalize()
    generate_gpc_mapping()

    print("Generated:")
    print(" -", OUT_BRANDS)
    print(" -", OUT_GS1)
    print(" -", OUT_PRODUCTS)
    print(" -", OUT_IMAGES)
    print(" -", OUT_NORMALIZE)
    print(" -", OUT_GPCMAP)


if __name__ == "__main__":
    main()
